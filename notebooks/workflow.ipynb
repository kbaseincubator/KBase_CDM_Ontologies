{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7714548c-2b29-4c6e-8d32-3c9274280789",
   "metadata": {},
   "source": [
    "## CDM ONTOLOGIES WORKFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b0498-54cd-480c-9d7e-96bfe6ce4141",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Full workflow"
   ]
  },
  {
   "cell_type": "code",
   "id": "44ffde61-3391-4db7-a1bf-bdd254974c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": "%%time\n# Updated workflow using the current cdm_ontologies package structure\nimport os\nfrom pathlib import Path\n\n# Get the repository root path\nrepo_path = Path(os.getcwd()).parent\n\n# Print workflow start\nprint(\"Starting CDM Ontologies Workflow...\")\nprint(f\"Repository path: {repo_path}\")\n\n# Run the complete workflow using the CLI\nimport subprocess\n\n# Run the complete workflow\nresult = subprocess.run(\n    [\"python\", \"-m\", \"cdm_ontologies.cli\", \"run-all\"],\n    cwd=repo_path,\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nif result.stderr:\n    print(\"STDERR:\", result.stderr)\n    \nif result.returncode == 0:\n    print(\"\\nWorkflow completed successfully!\")\nelse:\n    print(f\"\\nWorkflow failed with return code: {result.returncode}\")"
  },
  {
   "cell_type": "markdown",
   "id": "12e608d7-99fa-4d00-8cdd-235e0588112f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Worflow in seperate steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e31d686-e69a-4085-8888-08545ee08eeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Analyze and Download Core Ontologies"
   ]
  },
  {
   "cell_type": "code",
   "id": "210f3f80-b954-4b2c-a933-906b562a02dd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": "%%time\nimport os\nimport subprocess\nfrom pathlib import Path\n\n# Get the repository root path\nrepo_path = Path(os.getcwd()).parent\n\nprint(\"Analyzing Core Ontologies...\")\nprint(f\"Repository path: {repo_path}\")\n\n# Run analyze-core using the CLI\nresult = subprocess.run(\n    [\"python\", \"-m\", \"cdm_ontologies.cli\", \"analyze-core\"],\n    cwd=repo_path,\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nif result.stderr:\n    print(\"STDERR:\", result.stderr)"
  },
  {
   "cell_type": "markdown",
   "id": "f5810db4-bf44-4905-8de0-13156df573b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Analyze and Download Non-Core Ontologies"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5b35a0f-53f8-4b85-9149-2ba2d9a0efde",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": "%%time\nimport os\nimport subprocess\nfrom pathlib import Path\n\n# Get the repository root path\nrepo_path = Path(os.getcwd()).parent\n\nprint(\"Analyzing Non-Core Ontologies...\")\nprint(f\"Repository path: {repo_path}\")\n\n# Run analyze-non-core using the CLI\nresult = subprocess.run(\n    [\"python\", \"-m\", \"cdm_ontologies.cli\", \"analyze-non-core\"],\n    cwd=repo_path,\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nif result.stderr:\n    print(\"STDERR:\", result.stderr)"
  },
  {
   "cell_type": "markdown",
   "id": "87eb0912-a5e9-4e2f-9362-37f1cc18b553",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Recreate pseudo base versions"
   ]
  },
  {
   "cell_type": "code",
   "id": "704c982f-8ab8-47ca-9c95-bbdb1953c05a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "%%time\nimport os\nimport subprocess\nfrom pathlib import Path\n\n# Get the repository root path\nrepo_path = Path(os.getcwd()).parent\n\nprint(\"Creating Pseudo Base Ontologies...\")\nprint(f\"Repository path: {repo_path}\")\n\n# Run create-base using the CLI\nresult = subprocess.run(\n    [\"python\", \"-m\", \"cdm_ontologies.cli\", \"create-base\"],\n    cwd=repo_path,\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nif result.stderr:\n    print(\"STDERR:\", result.stderr)"
  },
  {
   "cell_type": "markdown",
   "id": "29336d90-3d6e-4289-87ae-9afcdc41a3dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Analyze the prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97848151-5f93-4219-bca9-787261a5eef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing ontologies in: /scratch/jplfaria/KBase_CDM_Ontologies/ontology_data_owl_core\n",
      "\n",
      "Analyzing ncbitaxon.owl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sys\n",
    "import os\n",
    "# Add the scripts directory to the Python path\n",
    "repo_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "scripts_path = os.path.join(repo_path, 'scripts')\n",
    "sys.path.append(scripts_path)\n",
    "# Import the prefix analyzer\n",
    "from analyze_prefixes import analyze_all_ontologies, generate_prefix_mapping\n",
    "# Define input directory path\n",
    "input_dir = os.path.join(repo_path, 'ontology_data_owl')\n",
    "# Run the analysis - NEED TO PASS BOTH ARGUMENTS\n",
    "print(f\"Analyzing ontologies in: {input_dir}\")\n",
    "results = analyze_all_ontologies(input_dir, repo_path)  # Added repo_path here!\n",
    "# Generate and save prefix mapping\n",
    "mapping_content = generate_prefix_mapping(results)\n",
    "mapping_file = os.path.join(repo_path, 'prefix_mapping.txt')\n",
    "with open(mapping_file, 'w') as f:\n",
    "    f.write(mapping_content)\n",
    "print(f\"\\nPrefix mapping file generated at: {mapping_file}\")\n",
    "# Print summary of analysis\n",
    "print(\"\\nSummary of analysis:\")\n",
    "for filename, data in results.items():\n",
    "    print(f\"\\n{filename}:\")\n",
    "    print(f\"  Declared prefixes: {len(data['prefixes'])}\")\n",
    "    additional_prefixes = set(data['prefix_to_iris'].keys()) - data['prefixes']\n",
    "    print(f\"  Potential additional prefixes needed: {len(additional_prefixes)}\")\n",
    "    if additional_prefixes:\n",
    "        print(\"  Additional prefixes:\")\n",
    "        for prefix in sorted(additional_prefixes):\n",
    "            iris = data['prefix_to_iris'][prefix]\n",
    "            if iris:\n",
    "                print(f\"    - {prefix}: {next(iter(iris))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c3736-eb5b-4461-9fe8-25c0e2b97d70",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Merge Ontologies"
   ]
  },
  {
   "cell_type": "code",
   "id": "4add5f0c-5009-4998-896f-2b95759caa2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "%%time\nimport os\nimport subprocess\nfrom pathlib import Path\nfrom threading import Thread\nimport time\nimport logging\n\n# Get the repository root path\nrepo_path = Path(os.getcwd()).parent\n\n# Set up logging for memory monitoring\nlogging.basicConfig(\n    filename=repo_path / 'logs' / 'merge_memory.log',\n    level=logging.INFO,\n    format='%(asctime)s - %(message)s'\n)\n\nprint(\"Merging Ontologies...\")\nprint(f\"Repository path: {repo_path}\")\nprint(\"Memory usage will be logged to logs/merge_memory.log\")\n\n# Start memory monitoring in background if needed\ndef monitor_memory():\n    \"\"\"Monitor memory usage during merge\"\"\"\n    while True:\n        try:\n            # Get memory info\n            mem_info = subprocess.check_output(['free', '-h']).decode('utf-8').split('\\n')\n            if len(mem_info) > 1:\n                mem_line = mem_info[1].split()\n                if len(mem_line) > 2:\n                    used_mem = mem_line[2]\n                    logging.info(f\"Memory used: {used_mem}\")\n            time.sleep(60)  # Log every minute\n        except:\n            break\n\n# Start monitoring thread\nmonitor_thread = Thread(target=monitor_memory, daemon=True)\nmonitor_thread.start()\n\n# Run merge using the CLI\nresult = subprocess.run(\n    [\"python\", \"-m\", \"cdm_ontologies.cli\", \"merge\"],\n    cwd=repo_path,\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nif result.stderr:\n    print(\"STDERR:\", result.stderr)\n    \nif result.returncode == 0:\n    print(\"\\nMerge completed successfully!\")\nelse:\n    print(f\"\\nMerge failed with return code: {result.returncode}\")"
  },
  {
   "cell_type": "markdown",
   "id": "9536cfaf-d481-4178-b12f-40f0ab1d24b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Semantic SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e34d064-8e8c-4b0b-839a-bf68fad15326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Python executable: /home/jplfaria/miniconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Current Python executable: {sys.executable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18854439-deba-4a5d-a494-c8899d2cabf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14759b18-0ce7-4424-b7c6-e77dc4480429",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: Could not find semsql installation location\n",
      "CPU times: user 24.9 ms, sys: 128 Î¼s, total: 25.1 ms\n",
      "Wall time: 398 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/jplfaria/KBase_CDM_Ontologies/scripts/create_semantic_sql_db.py\", line 24, in create_semantic_sql_db\n",
      "    raise Exception(\"Could not find semsql installation location\")\n",
      "Exception: Could not find semsql installation location\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the scripts directory to the Python path\n",
    "repo_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "scripts_path = os.path.join(repo_path, 'scripts')\n",
    "sys.path.append(scripts_path)\n",
    "\n",
    "# Import and run the database creation\n",
    "from create_semantic_sql_db import create_semantic_sql_db\n",
    "\n",
    "# Run the database creation with custom input filename\n",
    "create_semantic_sql_db(\n",
    "    repo_path,\n",
    "    input_owl_filename='eccode.owl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "id": "018db057-28e2-4ad1-8fa6-12b7dc25abff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "%%time\nimport os\nimport subprocess\nfrom pathlib import Path\n\n# Get the repository root path\nrepo_path = Path(os.getcwd()).parent\n\nprint(\"Creating Semantic SQL Database...\")\nprint(f\"Repository path: {repo_path}\")\n\n# Run create-db using the CLI\nresult = subprocess.run(\n    [\"python\", \"-m\", \"cdm_ontologies.cli\", \"create-db\"],\n    cwd=repo_path,\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nif result.stderr:\n    print(\"STDERR:\", result.stderr)\n    \nif result.returncode == 0:\n    print(\"\\nDatabase creation completed successfully!\")\nelse:\n    print(f\"\\nDatabase creation failed with return code: {result.returncode}\")"
  },
  {
   "cell_type": "markdown",
   "id": "a59e2574-b743-486e-9962-e6f04a591be5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Extract SQLite tables to .tsv"
   ]
  },
  {
   "cell_type": "code",
   "id": "1e7e717d-54f1-461b-8a12-7d7cbdfdf3c4",
   "metadata": {},
   "outputs": [],
   "source": "%%time\nimport os\nimport subprocess\nfrom pathlib import Path\n\n# Get the repository root path\nrepo_path = Path(os.getcwd()).parent\n\nprint(\"Extracting SQL Tables to TSV...\")\nprint(f\"Repository path: {repo_path}\")\n\n# Run extract-tables using the CLI\nresult = subprocess.run(\n    [\"python\", \"-m\", \"cdm_ontologies.cli\", \"extract-tables\"],\n    cwd=repo_path,\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nif result.stderr:\n    print(\"STDERR:\", result.stderr)\n    \nif result.returncode == 0:\n    print(\"\\nTable extraction completed successfully!\")\nelse:\n    print(f\"\\nTable extraction failed with return code: {result.returncode}\")"
  },
  {
   "cell_type": "code",
   "id": "6bc37b06-a091-44ef-95d2-09cd307aa80c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "%%time\nimport os\nimport subprocess\nfrom pathlib import Path\n\n# Get the repository root path\nrepo_path = Path(os.getcwd()).parent\n\nprint(\"Creating Parquet Files...\")\nprint(f\"Repository path: {repo_path}\")\n\n# Run create-parquet using the CLI\nresult = subprocess.run(\n    [\"python\", \"-m\", \"cdm_ontologies.cli\", \"create-parquet\"],\n    cwd=repo_path,\n    capture_output=True,\n    text=True\n)\n\nprint(result.stdout)\nif result.stderr:\n    print(\"STDERR:\", result.stderr)\n    \nif result.returncode == 0:\n    print(\"\\nParquet file creation completed successfully!\")\nelse:\n    print(f\"\\nParquet file creation failed with return code: {result.returncode}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bba10c-a198-4b79-964d-7c0895213f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}